from groq import Groq
from reflector import generate_reflection
from utils import filepreprocess
import os

# Set Groq API key
os.environ["GROQ_API_KEY"] = "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"
client = Groq()

def run_reflection_cycle(csv_file_path: str, problem_description: str, max_reflections: int = 3):
    """
    Runs an iterative reflection and refinement loop on the ML code generated by the original model.
    
    Parameters:
        csv_file_path (str): Path to the CSV file for training.
        problem_description (str): Business problem to solve, passed to the original model.
        max_reflections (int): The number of reflection cycles to run. Default is 3.
    
    Returns:
        str: The final refined ML code after reflection cycles.
    """
    
    # initial setup based on the inputs
    generation_prompt = (
        "You are an AI ML engineer. Generate Python code to solve the following business problem:\n"
        f"Business Problem: {problem_description}\n"
        f"CSV File: {csv_file_path}\n"
        "Generate appropriate ML code that handles preprocessing, feature selection, model training, and evaluation."
    )
    
    # initial code generation from Groq using the LLaMA model
    ml_code_response = client.chat.completions.create(
        messages=[{"role": "system", "content": generation_prompt}],
        model="llama3-70b-8192"
    ).choices[0].message.content
    
    print("Initial ML Code Generated:\n", ml_code_response)
    
    # Start reflection loop
    reflection_count = 0
    while reflection_count < max_reflections:
        print(f"\nReflection cycle {reflection_count + 1}...\n")
        
        # Generate reflection on the current ML code
        reflection_feedback = generate_reflection(ml_code_response, csv_file_path)
        print(f"Reflection Feedback:\n{reflection_feedback}")
        
        # Refine the ML code based on the feedback
        ml_code_refinement_prompt = (
            f"Refine the following ML code based on the feedback provided. Focus on improving the algorithm, "
            "feature selection, data preprocessing, and evaluation.\n\n"
            f"Reflection Feedback:\n{reflection_feedback}\n\n"
            f"Current ML Code:\n{ml_code_response}\n"
            "Provide a refined version of the code."
        )
        
        # Request refined code from Groq using the LLaMA model
        refined_code_response = client.chat.completions.create(
            messages=[{"role": "system", "content": ml_code_refinement_prompt}],
            model="llama3-70b-8192"
        ).choices[0].message.content
        
        # Update ML code for next cycle
        ml_code_response = refined_code_response
        print(f"Refined ML Code:\n{refined_code_response}")
        
        reflection_count += 1
    
    return ml_code_response
